# Long-tail-multiple-experts
Collection of papers that use multiple experts to solve long-tail classification.

# Classification

| Year | Conference | Title | Paper Link | Code Link |
|------|------------|-------|------------|-----------|
| 2023 | ICCV       | MDCS: More Diverse Experts with Consistency Self-Distillation for Long-Tailed Recognition | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MDCS_More_Diverse_Experts_with_Consistency_Self-distillation_for_Long-tailed_Recognition_ICCV_2023_paper.pdf) | [Code](https://github.com/fistyee/MDCS) |
| 2023 | ICCV       | Local and Global Logit Adjustments for Long-Tailed Learning | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Tao_Local_and_Global_Logit_Adjustments_for_Long-Tailed_Learning_ICCV_2023_paper.pdf) |
|~~~| | | | |
| 2022 | CVPR       | Nested Collaborative Learning for Long-Tailed Visual Recognition | [Paper](https://arxiv.org/pdf/2203.15359.pdf) | [Code](https://github.com/Bazinga699/NCL) |
| 2022 | NeurIPS    | Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition | [Paper](https://arxiv.org/pdf/2107.09249.pdf) | [Code](https://github.com/Vanint/SADE-AgnosticLT?tab=readme-ov-file) |
| 2022 | TPAMI      | ResLT: Residual Learning for Long-tailed Recognition | [Paper](https://arxiv.org/pdf/2101.10633.pdf) | [Code](https://github.com/jiequancui/ResLT) |
|~~~| | | | |
| 2021 | ICLR       | RIDE: Long-tailed Recognition by Routing Diverse Distribution-Aware Experts | [Paper](http://people.eecs.berkeley.edu/~xdwang/papers/ICLR2021_RIDE.pdf) | [Code](https://github.com/frank-xwang/RIDE-LongTailRecognition) |
| 2021 | ICCV       | ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot | [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_ACE_Ally_Complementary_Experts_for_Solving_Long-Tailed_Recognition_in_One-Shot_ICCV_2021_paper.pdf) | [Code](https://github.com/winterxx/ACE) |
|~~~| | | | |
| 2020 | ECCV       | Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification | [Paper](https://arxiv.org/abs/2001.01536) | [Code](https://github.com/xiangly55/LFME) |
